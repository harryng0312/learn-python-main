{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.lines as mlines\n","import matplotlib.animation as animation\n","import time\n","import struct\n","import tensorflow as tf\n","import random as rd\n","import pickle as pickle\n","\n","from array import array\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# my project\n","from module.conf import PROJECT_DIR\n","\n","# %matplotlib tk\n","%matplotlib inline\n","\n","tf.keras.backend.set_floatx('float64')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_images_labels(data_filepath) -> tuple:\n","    labels = []\n","    images = []\n","    for path in data_filepath:\n","        with open(file = path, mode=\"rb\") as f:\n","            dict_data = pickle.load(file=f, encoding=\"bytes\")\n","            dd = dict_data[b'data']\n","            for ind, val in enumerate(dd):\n","                label = dict_data[b'labels'][ind]\n","                img = np.asarray(dd[ind], dtype=np.uint8).reshape(3, 32, 32).transpose(1, 2, 0)\n","                labels.append(label)\n","                images.append(img)\n","                pass\n","            pass\n","        pass\n","    return np.asarray(images), np.asarray(labels)\n","\n","def load_data() -> tuple:\n","    cifar_path = \"/data/sample/cifar-10-batches-py\"\n","    label_name_filepath = \"\".join([PROJECT_DIR, cifar_path, \"/batches.meta\"])\n","    training_data_filepaths = [\n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_1\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_2\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_3\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_4\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_5\"]), \n","    ]\n","    test_data_filepaths = [\"\".join([PROJECT_DIR, cifar_path, \"/test_batch\"])]\n","    x_train, y_train = read_images_labels(training_data_filepaths[:])\n","    x_test, y_test = read_images_labels(test_data_filepaths)\n","    with open(file=label_name_filepath, mode=\"rb\") as f:\n","        label_names = pickle.load(file=f, encoding=\"bytes\")[b'label_names']\n","        pass\n","    return (x_train, y_train),(x_test, y_test), label_names\n","\n","(X_train, y_train), (X_test, y_test), label_names = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print(X_train.dtype)\n","X_train = X_train.astype(dtype=np.float64) / 255\n","X_test = X_test.astype(dtype=np.float64) / 255\n","# for i in range(len(X_train)): X_train[i] /= 255\n","# for i in range(len(X_test)): X_test[i] /= 255 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(shape=(32, 32, 3)))\n","#------------------------------------\n","# Conv Block 1: 32 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 2: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 3: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    \n","#------------------------------------\n","# Flatten the convolutional features.\n","#------------------------------------\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(512, activation='relu'))\n","model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n","# import os\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n","try:\n","  # Disable all GPUS\n","  tf.config.set_visible_devices([], 'GPU')\n","  visible_devices = tf.config.get_visible_devices()\n","  print(f\"visible_devices:{visible_devices}\")\n","  for device in visible_devices:\n","    assert device.device_type != 'GPU'\n","except Exception as e:\n","  # Invalid device or cannot modify virtual devices once initialized.\n","  print(e)\n","  pass\n","\n","# enable GPU\n","# physical_devices = tf.config.list_physical_devices('GPU')\n","# try:\n","#   # Disable first GPU\n","#   tf.config.set_visible_devices(physical_devices, 'GPU')\n","#   logical_devices = tf.config.list_logical_devices('GPU')\n","#   # Logical device was not created for first GPU\n","#   assert len(logical_devices) == len(physical_devices) - 1\n","# except:\n","#   # Invalid device or cannot modify virtual devices once initialized.\n","#   pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.categorical_crossentropy\n","loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","#               loss=tf.keras.losses.BinaryCrossentropy(),\n","#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.FalseNegatives()])\n","# model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","#               loss=loss_fn,\n","#               metrics=[\"accuracy\", \"mae\"])\n","model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=1e-3),\n","            loss=loss_fn,\n","            metrics=[\"accuracy\"])\n","\n","# model.compile(optimizer='rmsprop', \n","#               loss='sparse_categorical_crossentropy', \n","#               metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["**Do not fit on PC, let do it on cloud**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model.fit(x=X_train, y=y_train, epochs=32, batch_size=500, verbose=1)\n","# model.fit(x=X_train, y=y_train, epochs=16, batch_size=512, verbose=1)"]},{"cell_type":"markdown","metadata":{},"source":["Load from Saved Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = tf.keras.models.load_model(filepath=\"\".join([PROJECT_DIR, \"/data/cifar10-python/model\"]), compile=True)\n","# model.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate(X_test, y_test, verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = model.predict(x=np.asarray([X_test[0]], dtype=np.float64), verbose=1)\n","# result = tf.nn.softmax(result).numpy()\n","print(f\"{result}\")\n","print(f\"{result.argmax()} {y_test[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["c = 0\n","cp = 0\n","for i in range(100):\n","    test_indx = rd.randint(0, len(y_test)-1)\n","    x_test_ = np.asarray([X_test[test_indx]])\n","\n","    # test_indx = rd.randint(0, len(y_train)-1)\n","    # x_test_ = np.asarray([X_train[test_indx]])\n","\n","    result = model.predict(x=x_test_, verbose=0) \n","#     result = tf.nn.softmax(result).numpy()\n","    y_test_ = y_test\n","    if result.max() >= 0.5:\n","        if result.argmax() != y_test_[test_indx]:\n","            c+=1\n","            print(f\"- [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} {label_names[result.argmax()]} solve:{y_test_[test_indx]} {label_names[y_test_[test_indx]]}\")\n","    else:\n","        print(f\"can not predict:{test_indx}: {result.max()}\")\n","        cp+=1\n","print(f\"error: {c} can not pred:{cp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ind = 14\n","fig, ax = plt.subplots(figsize=(1.6, 1.2))\n","ax.imshow(X=X_train[ind])\n","plt.show()\n","print(f\"label[{y_train[ind]}]:{label_names[y_train[ind]]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.config.list_physical_devices()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["physical_devices = tf.config.list_physical_devices('GPU')\n","try:\n","  # Disable first GPU\n","  tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n","  logical_devices = tf.config.list_logical_devices('GPU')\n","  # Logical device was not created for first GPU\n","  assert len(logical_devices) == len(physical_devices) - 1\n","except:\n","  # Invalid device or cannot modify virtual devices once initialized.\n","  pass"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.config.list_physical_devices('GPU')"]},{"cell_type":"markdown","metadata":{},"source":["2. Resnet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","resnet_model = tf.keras.applications.ResNet50(input_shape=(32, 32, 3), include_top=True, weights=None, classes=10)\n","resnet_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","              loss=loss_fn,\n","              metrics=[\"accuracy\", \"mae\"])\n","\n","resnet_model.summary()\n","resnet_model.save()\n","# resnet_model.fit(x=X_train, y=y_train, epochs=5, batch_size=64, verbose=1)\n","\n","# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","# model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n","# model.fit(x_train, y_train, epochs=5, batch_size=64)"]}],"metadata":{"kernelspec":{"display_name":"learn-python","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
