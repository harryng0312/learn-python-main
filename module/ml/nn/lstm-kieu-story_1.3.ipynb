{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-02T17:54:58.814027Z","iopub.status.busy":"2023-11-02T17:54:58.813651Z","iopub.status.idle":"2023-11-02T17:54:58.820284Z","shell.execute_reply":"2023-11-02T17:54:58.819362Z","shell.execute_reply.started":"2023-11-02T17:54:58.813995Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.callbacks import LambdaCallback\n","from tensorflow.keras.models import Model, load_model, Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n","from tensorflow.keras.layers import LSTM, GRU\n","from tensorflow.keras.utils import get_file, to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import sys\n","import io\n","\n","from module.conf import PROJECT_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:54:58.824257Z","iopub.status.busy":"2023-11-02T17:54:58.823973Z","iopub.status.idle":"2023-11-02T17:54:58.834834Z","shell.execute_reply":"2023-11-02T17:54:58.833973Z","shell.execute_reply.started":"2023-11-02T17:54:58.824234Z"},"trusted":true},"outputs":[],"source":["# import os\n","# try:\n","#     # Disable all GPUS\n","#     tf.config.set_visible_devices([], 'GPU')\n","#     visible_devices = tf.config.get_visible_devices()\n","#     for device in visible_devices:\n","#         assert device.device_type != 'GPU'\n","# except:\n","#     # Invalid device or cannot modify virtual devices once initialized.\n","#     pass\n","strategy  = tf.distribute.get_strategy()\n","# with strategy.scope():\n","with tf.device(\"/cpu:0\"):\n","    print(f\"{tf.config.list_physical_devices() }\")\n","# print(f\"{tf.config.list_physical_devices('GPU') }\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizer, BertModel\n","phoBERT: BertModel = AutoModel.from_pretrained(PROJECT_DIR + \"/models/phobert-base\")\n","custokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(PROJECT_DIR + \"/models/phobert-base\", use_fast=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:54:58.837048Z","iopub.status.busy":"2023-11-02T17:54:58.836409Z","iopub.status.idle":"2023-11-02T17:54:58.846865Z","shell.execute_reply":"2023-11-02T17:54:58.846120Z","shell.execute_reply.started":"2023-11-02T17:54:58.837015Z"},"trusted":true},"outputs":[],"source":["data_file_path = PROJECT_DIR + '/data/lstm/truyen_kieu_data_pre.txt'\n","\n","def load_data() -> tuple:\n","    rs: map = {}\n","    ls = []\n","    tmp_line = \"\"\n","    with open(file=data_file_path, mode=\"rt\") as i_f:\n","        count = 0\n","        for line in i_f:\n","            line = line.strip()\n","            if \"\" == line: continue\n","            if count % 2 == 0 and tmp_line!=\"\":\n","                tokens = custokenizer.tokenize(tmp_line.strip())\n","                num_line = custokenizer.convert_tokens_to_ids(tokens=tokens)\n","                ls.append(num_line)\n","                # ls.append(tmp_line.strip())\n","                tmp_line = \"\"\n","                for num in num_line:\n","                    if num not in rs: rs[num] = 0\n","                    rs[num] += 1\n","                    pass\n","                pass\n","            tmp_line += \" \"+line\n","            count+=1\n","        pass\n","    return rs, ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:54:58.848591Z","iopub.status.busy":"2023-11-02T17:54:58.848328Z","iopub.status.idle":"2023-11-02T17:54:58.890762Z","shell.execute_reply":"2023-11-02T17:54:58.890034Z","shell.execute_reply.started":"2023-11-02T17:54:58.848569Z"},"trusted":true},"outputs":[],"source":["bag_of_words, coupled_lines = load_data()\n","m = len(coupled_lines)\n","max_input_len = max(len(coupled_line) for coupled_line in coupled_lines)\n","print(f\"max_input_len: {max_input_len}\")\n","# bag_of_words[\"|\"] = len(coupled_lines)\n","# bag_of_words[\"\\\\\"] = len(coupled_lines) - 1\n","# ix_to_word = [*bag_of_words.keys()]\n","# keys = [*bag_of_words.keys()]\n","# word_to_ix = { w:i for i,w in enumerate(keys) }\n","# ix_to_word = { i:w for i,w in enumerate(keys) }\n","# coupled_lines\n","# values = [*bag_of_word.values()]\n","# sum(values)\n","# len(bag_of_words)\n","# ix_to_word\n","total_word = len(bag_of_words)\n","max_word_num = max(k for k in bag_of_words)\n","\n","X_train = np.zeros(shape=(m, max_input_len,))\n","for i in range(m):\n","    line = coupled_lines[i]\n","    line_ids = custokenizer.convert_tokens_to_ids(line)\n","    for j in range(len(line_ids)):\n","        X_train[i, j] = line_ids[j]\n","        pass\n","    pass\n","print(f\"X_train.shape:{X_train.shape}\")\n","delta_line = 1\n","# y_train = [line[5] for line in X_train[delta_line:]]\n","y_train = [*X_train[delta_line:]]\n","y_train = [e[0] for e in y_train]\n","for c in range(delta_line):\n","    y_train.append(X_train[c][0])\n","    pass\n","\n","\n","\n","print(f\"total_word:{total_word} max_word_num:{max_word_num}\")\n","# max_input_len = 14\n","X_train = np.asarray(a=X_train, dtype=int)\n","y_train = np.asarray(y_train) #.reshape(X_train.shape[0], 1)\n","# print(y_train.shape)\n","y_train = to_categorical(y_train, num_classes=total_word, dtype=int)\n","print(f\"y_train:{y_train.shape}\")\n","# X_train.shape\n","# y_train.shape\n","# len(coupled_lines[0].split())\n","# coupled_lines"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T18:04:30.197767Z","iopub.status.busy":"2023-11-02T18:04:30.197398Z","iopub.status.idle":"2023-11-02T18:04:31.681802Z","shell.execute_reply":"2023-11-02T18:04:31.680910Z","shell.execute_reply.started":"2023-11-02T18:04:30.197737Z"},"trusted":true},"outputs":[],"source":["model: tf.keras.Model = tf.keras.Sequential(name=\"LSTM-RNN\")\n","# input = tf.keras.layers.Input(shape=(15))\n","# embedding_1 = tf.keras.layers.Embedding(input_dim=total_word, output_dim=2048, input_length=max_input_len)\n","embedding_1 = tf.keras.layers.Embedding(input_dim=total_word, output_dim=2048, input_length=max_input_len)\n","bidrect_1 = tf.keras.layers.Bidirectional(\\\n","    layer=tf.keras.layers.LSTM(units=max_input_len*32, return_sequences=True, go_backwards=False),\\\n","    backward_layer=tf.keras.layers.LSTM(units=max_input_len*24, return_sequences=True, go_backwards=True))\n","dropout = tf.keras.layers.Dropout(rate=0.4)\n","bidrect_2 = tf.keras.layers.Bidirectional(layer=tf.keras.layers.LSTM(units=max_input_len*16, return_sequences=True))\n","dropout = tf.keras.layers.Dropout(rate=0.6)\n","lstm = tf.keras.layers.LSTM(units=max_input_len*16)\n","# output_1 = tf.keras.layers.Dense(units=total_word, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-2))\n","# output_2 = tf.keras.layers.Dense(units=total_word, activation=\"softmax\")\n","output_1 = tf.keras.layers.Dense(units=total_word, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-2))\n","output_2 = tf.keras.layers.Dense(units=total_word, activation=\"softmax\")\n","\n","# model.add(input)\n","model.add(embedding_1)\n","# model.add(bidrect_1)\n","# model.add(bidrect_2)\n","# model.add(dropout)\n","model.add(lstm)\n","model.add(dropout)\n","model.add(output_1)\n","model.add(output_2)\n","\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T18:04:35.596926Z","iopub.status.busy":"2023-11-02T18:04:35.596198Z","iopub.status.idle":"2023-11-02T18:04:35.611584Z","shell.execute_reply":"2023-11-02T18:04:35.610541Z","shell.execute_reply.started":"2023-11-02T18:04:35.596893Z"},"trusted":true},"outputs":[],"source":["tf.config.set_soft_device_placement(True) \n","# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","# loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","loss_fn = tf.keras.losses.categorical_crossentropy\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","#               loss=loss_fn,\n","#               metrics=[\"accuracy\"])\n","model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","              loss=loss_fn,\n","              metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T18:04:39.498985Z","iopub.status.busy":"2023-11-02T18:04:39.498168Z","iopub.status.idle":"2023-11-02T18:06:19.418415Z","shell.execute_reply":"2023-11-02T18:06:19.417399Z","shell.execute_reply.started":"2023-11-02T18:04:39.498950Z"},"trusted":true},"outputs":[],"source":["# strategy  = tf.distribute.get_strategy()\n","# if model is not None: del model\n","tf.keras.backend.clear_session()\n","with strategy.scope():\n","# with tf.device('/cpu:0'):\n","# with tf.device(\"/gpu:0\"):\n","    model.fit(x=X_train, y=y_train, validation_split=0.1, epochs=160, batch_size=128, verbose=1)\n","#     model.fit(x=X_train, y=y_train, epochs=75, batch_size=64, verbose=1)\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T17:56:14.388773Z","iopub.status.busy":"2023-11-02T17:56:14.388425Z","iopub.status.idle":"2023-11-02T17:56:24.446894Z","shell.execute_reply":"2023-11-02T17:56:24.445686Z","shell.execute_reply.started":"2023-11-02T17:56:14.388740Z"},"trusted":true},"outputs":[],"source":["model.save('/kaggle/working/kieu_story')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:56:24.447968Z","iopub.status.idle":"2023-11-02T17:56:24.448350Z","shell.execute_reply":"2023-11-02T17:56:24.448195Z","shell.execute_reply.started":"2023-11-02T17:56:24.448177Z"},"trusted":true},"outputs":[],"source":["import os\n","import subprocess\n","from IPython.display import FileLink, display\n","download_file_name = 'kieu_story'\n","path = f\"/kaggle/working/{download_file_name}\"\n","zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n","command = f\"zip {zip_name} {path} -r\"\n","result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","if result.returncode != 0:\n","    print(\"Unable to run zip command!\")\n","    print(result.stderr)\n","display(FileLink(f'{download_file_name}.zip'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-02T18:06:46.694886Z","iopub.status.busy":"2023-11-02T18:06:46.694510Z","iopub.status.idle":"2023-11-02T18:06:52.910791Z","shell.execute_reply":"2023-11-02T18:06:52.909809Z","shell.execute_reply.started":"2023-11-02T18:06:46.694855Z"},"trusted":true},"outputs":[],"source":["import random as rd\n","\n","num_of_word = 14 * 4\n","seed = \"đàn bà dễ có mấy tay đời xưa mấy mặt đời này mấy gan\"\n","# seed_arr = [ix_to_word[rd.randint(0, total_word)] for _ in range(14)]\n","# seed_arr[11] = seed_arr[5]\n","# seed = \" \".join(seed_arr).strip()\n","seed_arr_tokens = custokenizer.tokenize(seed)\n","# seed_sequence = [word_to_ix[word] for word in seed_arr]\n","seed_arr_ids: list = custokenizer.convert_tokens_to_ids(seed_arr_tokens)\n","# if len(seed_arr_ids) < max_input_len: sedd_arr_ids.append(0)\n","for i in range(max_input_len - len(seed_arr_ids)):\n","    seed_arr_ids.append(0)\n","    pass\n","rs = \"\"\n","for i in range(num_of_word):\n","    if i % max_input_len == 0 and i != 0:\n","#         seed_sequence.append(\" \\\\\")\n","        rs += (\"\\n\")\n","#         continue\n","    if i % max_input_len == 6:\n","#         seed_sequence.append(\" | \")\n","        rs += (\"\\n\")\n","#         continue\n","    prediction = model.predict(np.array([seed_arr_ids]), verbose=\"0\")\n","    predicted_word_idx = np.argmax(prediction)\n","#     predicted_word = ix_to_word[predicted_word_idx]\n","#     predicted_word = \n","    \n","    seed_arr_ids.append(predicted_word_idx)\n","    \n","    if len(seed_arr_ids) % 6 == 0:\n","        seed_arr_ids = seed_arr_ids[6:]\n","#     seed += \" \" + predicted_word\n","#     rs += \" \" + predicted_word\n","#     rs += \" \".join(custokenizer.convert_ids_to_tokens(seed_arr_ids),)\n","    print(seed_arr_ids)\n","    print(custokenizer.convert_ids_to_tokens(seed_arr_ids))\n","\n","print(rs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoModel, AutoTokenizer\n","\n","phoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","custokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
