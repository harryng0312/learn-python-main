{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file, to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "strategy  = tf.distribute.get_strategy()\n",
    "with strategy.scope():\n",
    "    print(f\"{tf.config.list_physical_devices('GPU') }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 2394)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_file = PROJECT_DIR + \"/data/lstm/truyen_kieu_data.txt\"\n",
    "pre_file = PROJECT_DIR + \"/data/lstm/truyen_kieu_data_pre.txt\"\n",
    "\n",
    "def pre_data() -> None:\n",
    "    with open(file=org_file, mode=\"rt\") as i_f:\n",
    "        with open(file=pre_file, mode=\"wt\") as o_f:\n",
    "            for line in i_f:\n",
    "                if \"\" == line.strip(): continue\n",
    "                a_word = line.split()\n",
    "                pre_word = \"\"\n",
    "                for word in a_word:\n",
    "                    tmp_line = \"\".join(filter(str.isalpha, word.strip()))\n",
    "                    pre_word = \" \".join((pre_word, tmp_line))\n",
    "                    pass\n",
    "                pre_word = pre_word.strip().lower()\n",
    "                o_f.write(f\"{pre_word}\\n\") \n",
    "                pass\n",
    "            pass\n",
    "        pass\n",
    "    return\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    rs: map = {}\n",
    "    ls = []\n",
    "    tmp_line = \"\"\n",
    "    with open(file=pre_file, mode=\"rt\") as i_f:\n",
    "        count = 0\n",
    "        for line in i_f:\n",
    "            line = line.strip()\n",
    "            if \"\" == line: continue\n",
    "            if count % 2 == 0 and tmp_line!=\"\":\n",
    "                ls.append(tmp_line.strip())\n",
    "                tmp_line = \"\"\n",
    "                pass\n",
    "            tmp_line += (\" | \" if tmp_line!=\"\" else \"\") + line\n",
    "            a_word = line.split()\n",
    "            for word in a_word:\n",
    "                if word not in rs:\n",
    "                    rs[word] = 0\n",
    "                    pass\n",
    "                rs[word]+=1\n",
    "                pass\n",
    "            pass\n",
    "            count+=1\n",
    "        pass\n",
    "    return rs, ls\n",
    "\n",
    "pre_data()\n",
    "bag_of_words, coupled_lines = load_data()\n",
    "bag_of_words[\"|\"] = len(coupled_lines)\n",
    "# ix_to_word = [*bag_of_words.keys()]\n",
    "keys = [*bag_of_words.keys()]\n",
    "word_to_ix = { w:i for i,w in enumerate(keys) }\n",
    "ix_to_word = { i:w for i,w in enumerate(keys) }\n",
    "# coupled_lines\n",
    "# values = [*bag_of_word.values()]\n",
    "# sum(values)\n",
    "# len(bag_of_words)\n",
    "# ix_to_word\n",
    "X_train = [[word_to_ix[word] for word in coupled_line.split()] for coupled_line in coupled_lines]\n",
    "\n",
    "delta_line = 4\n",
    "# y_train = [line[5] for line in X_train[delta_line:]]\n",
    "y_train = [*X_train[delta_line:]]\n",
    "y_train = [e[5] for e in y_train]\n",
    "for c in range(delta_line):\n",
    "    y_train.append(X_train[c][5])\n",
    "    pass\n",
    "\n",
    "total_word = len(bag_of_words)\n",
    "max_input_len = 15\n",
    "X_train = np.asarray(a=X_train, dtype=int)\n",
    "y_train = np.asarray(y_train) #.reshape(X_train.shape[0], 1)\n",
    "# print(y_train.shape)\n",
    "y_train = to_categorical(y_train, num_classes=total_word, dtype=int)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: tf.keras.Model = tf.keras.Sequential(name=\"LSTM-RNN\")\n",
    "# input = tf.keras.layers.Input(shape=(15))\n",
    "embedding_1 = tf.keras.layers.Embedding(input_dim=total_word, output_dim=1024, input_length=max_input_len)\n",
    "bidrect_1 = tf.keras.layers.Bidirectional(\\\n",
    "    layer=tf.keras.layers.LSTM(units=160, return_sequences=True, go_backwards=False),\\\n",
    "    backward_layer=tf.keras.layers.LSTM(units=240, return_sequences=True, go_backwards=True))\n",
    "bidrect_2 = tf.keras.layers.Bidirectional(layer=tf.keras.layers.LSTM(units=256, return_sequences=True))\n",
    "dropout = tf.keras.layers.Dropout(rate=0.2)\n",
    "lstm = tf.keras.layers.LSTM(units=128)\n",
    "output_1 = tf.keras.layers.Dense(units=(total_word + 1)/2, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "output_2 = tf.keras.layers.Dense(units=total_word, activation=\"softmax\")\n",
    "\n",
    "# model.add(input)\n",
    "model.add(embedding_1)\n",
    "# model.add(bidrect_1)\n",
    "model.add(bidrect_2)\n",
    "model.add(dropout)\n",
    "model.add(lstm)\n",
    "model.add(output_1)\n",
    "model.add(output_2)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_soft_device_placement(True) \n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy  = tf.distribute.get_strategy()\n",
    "with strategy.scope():\n",
    "# with tf.device('/cpu:0'):\n",
    "# with tf.device(\"/gpu:0\"):\n",
    "    model.fit(x=X_train, y=y_train, epochs=160, batch_size=64, verbose=1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_couple_line = 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
