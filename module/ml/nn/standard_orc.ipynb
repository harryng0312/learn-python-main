{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as img\n",
    "import time\n",
    "# import struct\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "# import cv2\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "\n",
    "from array import array\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "# %matplotlib tk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/data/standard-ocr\"\n",
    "training_images_filepath = \"\".join([PROJECT_DIR, data_path, \"/data/training_data\"])\n",
    "test_images_filepath = \"\".join([PROJECT_DIR, data_path, \"/data/testing_data\"])\n",
    "\n",
    "def read_images_labels(images_filepath) -> tuple:\n",
    "    labels = []\n",
    "    images = []\n",
    "    dir_names: list = os.listdir(path=images_filepath)\n",
    "    dir_name_labels = []\n",
    "    for dir_name in dir_names:\n",
    "        if dir_name[0].isalnum():\n",
    "            dir_name_labels.append(dir_name)\n",
    "            pass\n",
    "        pass\n",
    "    dir_name_labels.sort()\n",
    "    print(f\"{(dir_name_labels)}\")\n",
    "    count = 0\n",
    "    data_sample_count = 0\n",
    "    for dir_name_label in dir_name_labels:\n",
    "        img_file_path = \"/\".join((images_filepath, dir_name_label,))\n",
    "        # print(f\"{img_file_path}\")\n",
    "        # labels.append(count)\n",
    "        # tmp_data_img = []\n",
    "        for root, dir, file in os.walk(top=img_file_path):\n",
    "            # print(f\"{root}/{file}\")\n",
    "            for file_name in file:\n",
    "                # print(f\"{root}/{file_name}\")\n",
    "                # arr_img: np.ndarray = img.imread(fname=f\"{root}/{file_name}\")\n",
    "                with Image.open(fp=f\"{root}/{file_name}\", mode=\"r\") as img:\n",
    "                    # tmp_data_img.append(img.copy())\n",
    "                    labels.append(count)\n",
    "                    images.append(img.copy())\n",
    "                    data_sample_count+=1\n",
    "                    pass\n",
    "                pass\n",
    "            pass\n",
    "        # images.append(tmp_data_img)\n",
    "        count+=1\n",
    "        pass\n",
    "    return images, labels, data_sample_count\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    x_train, y_train, sample_train_count = read_images_labels(training_images_filepath)\n",
    "    x_test, y_test, sample_test_count = read_images_labels(test_images_filepath)\n",
    "    return (x_train, y_train, sample_train_count),(x_test, y_test, sample_test_count)\n",
    "\n",
    "(X_train_img, y_train, sample_train_count), (X_test_img, y_test, sample_test_count) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Resize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(X: list) -> list:\n",
    "    \"\"\"\n",
    "    X: list of nd.array for image\n",
    "    \"\"\"\n",
    "    rs = []\n",
    "    for arr_img in X:\n",
    "        arr_img = arr_img.convert(\"L\")\n",
    "        ndarr_img = np.asarray(arr_img.resize(size=(30, 40)))\n",
    "        # arr.append(ndarr_img)\n",
    "        rs.append(ndarr_img)\n",
    "        pass\n",
    "    return rs\n",
    "\n",
    "# print(f\"{X_train_img[0]}\")\n",
    "# len(X_train_img[1(])\n",
    "# X_train_img[0][0].size\n",
    "# plt.imshow(X=np.asarray(X_train_img[0][0]), cmap=\"gray\")\n",
    "X_train = np.asarray(resize_data(X=X_train_img))/255\n",
    "X_test  = np.asarray(resize_data(X=X_test_img))/255\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 20628 1008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16ea64d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"{y_train[2250]} {sample_train_count} {sample_test_count}\")\n",
    "plt.imshow(X=X_train[2250], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Train by my NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: tf.keras.models.Model = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Flatten(input_shape=(40, 30,)),\n",
    "    # tf.keras.layers.Dense(units=25, activation=tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dense(units=15, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=36, activation=tf.keras.activations.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_19 (Conv2D)          (None, 40, 30, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 20, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 20, 15, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 10, 7, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 10, 7, 128)        73856     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 5, 3, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               983552    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 35)                17955     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,094,179\n",
      "Trainable params: 1,094,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(40, 30, 1)))\n",
    "#------------------------------------\n",
    "# Conv Block 1: 32 Filters, MaxPool.\n",
    "#------------------------------------\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#------------------------------------\n",
    "# Conv Block 2: 64 Filters, MaxPool.\n",
    "#------------------------------------\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#------------------------------------\n",
    "# Conv Block 3: 64 Filters, MaxPool.\n",
    "#------------------------------------\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "#------------------------------------\n",
    "# Flatten the convolutional features.\n",
    "#------------------------------------\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(35, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.FalseNegatives()])\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\", \"mae\"])\n",
    "# model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-4),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\", \"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "207/207 [==============================] - 4s 17ms/step - loss: 13876.0322 - accuracy: 0.4606 - mae: 17.4730\n",
      "Epoch 2/12\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 15169.6045 - accuracy: 0.4983 - mae: 17.4730\n",
      "Epoch 3/12\n",
      "207/207 [==============================] - 3s 17ms/step - loss: 27244.6562 - accuracy: 0.4615 - mae: 17.4730\n",
      "Epoch 4/12\n",
      " 40/207 [====>.........................] - ETA: 2s - loss: 36546.9258 - accuracy: 0.4470 - mae: 17.2089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# X_train.shape\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/engine/training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/utils/tf_utils.py:680\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    678\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/utils/tf_utils.py:673\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    671\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 673\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    674\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \n\u001b[1;32m   1139\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1126\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1125\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1127\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# X_train.shape\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 32x32; Received: input_shape=(30, 40, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mResNet152(input_shape\u001b[39m=\u001b[39;49m(\u001b[39m30\u001b[39;49m, \u001b[39m40\u001b[39;49m, \u001b[39m1\u001b[39;49m), include_top\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, classes\u001b[39m=\u001b[39;49m\u001b[39m35\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mlegacy\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m               loss\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hiepnq/Working/training/python/learn-python/module/ml/nn/standard_orc.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/applications/resnet.py:591\u001b[0m, in \u001b[0;36mResNet152\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m     x \u001b[39m=\u001b[39m stack1(x, \u001b[39m256\u001b[39m, \u001b[39m36\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv4\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    589\u001b[0m     \u001b[39mreturn\u001b[39;00m stack1(x, \u001b[39m512\u001b[39m, \u001b[39m3\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconv5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 591\u001b[0m \u001b[39mreturn\u001b[39;00m ResNet(\n\u001b[1;32m    592\u001b[0m     stack_fn,\n\u001b[1;32m    593\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    594\u001b[0m     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    595\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mresnet152\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    596\u001b[0m     include_top,\n\u001b[1;32m    597\u001b[0m     weights,\n\u001b[1;32m    598\u001b[0m     input_tensor,\n\u001b[1;32m    599\u001b[0m     input_shape,\n\u001b[1;32m    600\u001b[0m     pooling,\n\u001b[1;32m    601\u001b[0m     classes,\n\u001b[1;32m    602\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    603\u001b[0m )\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/applications/resnet.py:159\u001b[0m, in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m as true, `classes` should be 1000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[1;32m    160\u001b[0m     input_shape,\n\u001b[1;32m    161\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m,\n\u001b[1;32m    162\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m    163\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[1;32m    164\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[1;32m    165\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[0;32m~/conda/envs/learn-python/lib/python3.11/site-packages/keras/applications/imagenet_utils.py:408\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m                 )\n\u001b[1;32m    405\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m                 input_shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m min_size\n\u001b[1;32m    407\u001b[0m             ) \u001b[39mor\u001b[39;00m (input_shape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_size):\n\u001b[0;32m--> 408\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    409\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mInput size must be at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39m; Received: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m                 )\n\u001b[1;32m    413\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m require_flatten:\n",
      "\u001b[0;31mValueError\u001b[0m: Input size must be at least 32x32; Received: input_shape=(30, 40, 1)"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.applications.ResNet152(input_shape=(30, 40, 1), include_top=True, weights=None, classes=35)\n",
    "# model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\", \"mae\"])\n",
    "\n",
    "# model.summary()\n",
    "# model.save()\n",
    "# resnet_model.fit(x=X_train, y=y_train, epochs=5, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 591ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"{y_test[1000]} {X_test[1000].shape}\")\n",
    "rs = model.predict(x=np.asarray([X_test[1000]]))\n",
    "rs.argmax()\n",
    "# plt.imshow(X_test[1000], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 1s - loss: 1319173703598080.0000 - accuracy: 0.6954 - mae: 17.4730 - 657ms/epoch - 21ms/step\n",
      "- [14]:img[1002]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:13 solve:35\n",
      "- [15]:img[310]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:13 solve:11\n",
      "- [17]:img[469]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [29]:img[933]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:23 solve:33\n",
      "- [31]:img[452]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:16\n",
      "- [32]:img[456]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [34]:img[452]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:16\n",
      "- [35]:img[545]:[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:3 solve:19\n",
      "- [37]:img[456]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [41]:img[133]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:10 solve:4\n",
      "- [42]:img[686]:[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:0 solve:24\n",
      "- [47]:img[675]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:24\n",
      "- [54]:img[996]:[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:3 solve:35\n",
      "- [60]:img[442]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:14 solve:15\n",
      "- [61]:img[471]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [63]:img[757]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:23 solve:27\n",
      "- [65]:img[694]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:24\n",
      "- [67]:img[682]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:24\n",
      "- [71]:img[476]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:22 solve:17\n",
      "- [76]:img[600]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:30 solve:21\n",
      "- [81]:img[471]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [82]:img[469]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:12 solve:16\n",
      "- [83]:img[538]:[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:3 solve:19\n",
      "- [86]:img[699]:[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:0 solve:24\n",
      "- [94]:img[687]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:24\n",
      "- [95]:img[481]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:22 solve:17\n",
      "- [96]:img[311]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:17 solve:11\n",
      "- [97]:img[267]:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:26 solve:9\n",
      "- [98]:img[543]:[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "pred:1.0\n",
      "predict:3 solve:19\n",
      "error: 29 can not pred:0\n"
     ]
    }
   ],
   "source": [
    "eval_rs = model.evaluate(X_test, y_test, verbose=2)\n",
    "# print(f\"{eval_rs}\")\n",
    "c = 0\n",
    "cp = 0\n",
    "for i in range(100):\n",
    "    test_indx = rd.randint(0, len(y_test)-1)\n",
    "    x_test_ = np.asarray([X_test[test_indx]])\n",
    "\n",
    "    # test_indx = rd.randint(0, len(y_train)-1)\n",
    "    # x_test_ = np.asarray([X_train[test_indx]])\n",
    "\n",
    "    result = model.predict(x=x_test_, verbose=0)\n",
    "    # result = tf.nn.softmax(result).numpy()\n",
    "    y_test_ = y_test\n",
    "    if result.max() >= 0.5:\n",
    "        if result.argmax() != y_test_[test_indx]:\n",
    "            c+=1\n",
    "            print(f\"- [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} solve:{y_test_[test_indx]}\")\n",
    "    else:\n",
    "        print(f\"can not predict:{test_indx}: {result.max()}\")\n",
    "        cp+=1\n",
    "print(f\"error: {c} can not pred:{cp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[679], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
