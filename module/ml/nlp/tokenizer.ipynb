{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, AutoModel, AutoTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizer, BertModel\n",
    "from module.conf import PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "# custokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "\n",
    "phoBERT: BertModel = AutoModel.from_pretrained(PROJECT_DIR + \"/models/phobert-base\")\n",
    "custokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(PROJECT_DIR + \"/models/phobert-base\", use_fast=False)\n",
    "\n",
    "# phoBERT.save_pretrained(PROJECT_DIR + \"/models/phobert-base\")\n",
    "# custokenizer.save_pretrained(PROJECT_DIR + \"/models/phobert-base\")\n",
    "# custokenizer.save_pretrained(PROJECT_DIR + \"/models/phobert-base\")\n",
    "\n",
    "# phoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "# custokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------add token ('\\n') to enter lines --------#\n",
    "custokenizer.add_tokens('\\n')\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"vợ chàng quỷ quái tinh ma phen này kẻ cắp bà già gặp nhau\"\n",
    "print('Sequences start:', line)\n",
    "#-------------encode --------------#\n",
    "tokens = custokenizer.encode(line)\n",
    "tokens[12]+=2000\n",
    "print(f'tokens list: {tokens} | {len(tokens)}')\n",
    "#-----------Decode ngược lại thành câu từ chuỗi index token---------------#\n",
    "tmp = custokenizer.decode(tokens)\n",
    "tmp = tmp.removeprefix(\"<s>\").removesuffix(\"</s>\").strip()\n",
    "print(f'decode ngược lại tokenize: {tmp} - {type(tmp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"đầu lòng hai ả tố nga Thúy Kiều là chị em là Thúy Vân\"\n",
    "# custokenizer.tokenize(text=line)\n",
    "encoded_text = custokenizer.encode(text=line)\n",
    "print(f\"encoded_text: {encoded_text} len: {len(encoded_text)}\")\n",
    "tokenized_text = custokenizer.tokenize(line)\n",
    "print(f\"tokenized_text: {tokenized_text} len: {len(tokenized_text)}\")\n",
    "\n",
    "ids = custokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"ids: {ids} len: {len(ids)}\")\n",
    "\n",
    "tokens = custokenizer.convert_ids_to_tokens(ids=ids)\n",
    "\n",
    "strs = custokenizer.convert_tokens_to_string(tokens=tokens)\n",
    "print(f\"strs: {strs} len: {len(strs)}\")\n",
    "# tokens = custokenizer.convert_tokens_to_string(ids)\n",
    "# print(f\"tokens: {tokens}\")\n",
    "\n",
    "tokenized_text_ids = custokenizer.convert_tokens_to_ids(custokenizer.tokenize(line))\n",
    "print(f\"tokens: {tokenized_text_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Create Dataset----------------#\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from filelock import FileLock\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach\n",
    "    soon.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tokenizers : is pretrain tokenizer of PhoBERT\n",
    "    file_path  : path to file train, test\n",
    "    block_size : size of 1 block , optinal\n",
    "    cache_dir  : just load 1 once and saved\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir if cache_dir is not None else directory,\n",
    "            \"cached_lm_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # -----------Make sure only the first process in distributed training processes the dataset,----------------#\n",
    "        # ---------------------------------------and the others will use the cache------------------------#\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "\n",
    "                self.examples = []\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                #-----convert text to tokenizers----------------------------#\n",
    "                '''\n",
    "                1. Convert word -> subword (tokenizer.tokenize(text))\n",
    "                2. COnvert subword -> number (tokenizer.convert_tokens_to_ids)\n",
    "                '''\n",
    "                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "                # ------------- Truncate in block of block_size-----------------#\n",
    "                #-----------Beacuse add_token('\\n') -> inds = 64001------------#\n",
    "                #--------If len(block_size)>56 so cut and add_special_tokens (<s>, </s>)---------------#\n",
    "                i = 0\n",
    "                while i < len(tokenized_text) - block_size + 1:\n",
    "                    inds = tokenized_text[i : i + block_size]\n",
    "                    for j in range(0, len(inds)):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[j+1:] #remove the first \\n\n",
    "                            break\n",
    "                    for j in range(len(inds)-1, 0, -1):\n",
    "                        if inds[j]==64001:\n",
    "                            inds = inds[:j-1] #remove \\n\n",
    "                            break\n",
    "                    i += len(inds)\n",
    "                    self.examples.append(\n",
    "                        tokenizer.build_inputs_with_special_tokens(inds)\n",
    "                    )\n",
    "                    \n",
    "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
    "                # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "                # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
    " #-----------Load dataset-----------------------#\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling, LineByLineWithSOPTextDataset\n",
    "\n",
    "def load_dataset(train_path, test_path, custokenizer):\n",
    "    train_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size= 56)#256\n",
    "     \n",
    "    test_dataset = PoemDataset(\n",
    "          tokenizer=custokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=56)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=custokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,custokenizer)\n",
    "#-----------Test dataloader----------------#\n",
    "print(len(test_dataset))\n",
    "print(len(train_dataset))\n",
    "#-------------Test decode to sentence ---------------#\n",
    "print(custokenizer.decode(test_dataset[7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Config, GPT2LMHeadModel\n",
    "#--------------------------Load  pretrain model GPT-2--------------------#\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Random weights => fine-turning model\n",
    "rand_weight = torch.rand(model_gpt2.lm_head.weight.shape)\n",
    "print(rand_weight)\n",
    "model_gpt2.lm_head.weight = torch.nn.parameter.Parameter(rand_weight)\n",
    "'''\n",
    "Because GPT2 has vocabulary_size 50257 and (wte): Embedding(50257, 768)\n",
    "So  convert vocabulary_size= 64002, Embedding(64002, 768)\n",
    "'''\n",
    "task_gpt2 = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "config_gpt2 = configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task_gpt2,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "model_gpt2 = GPT2LMHeadModel(config_gpt2)\n",
    "model_gpt2\n",
    "#save model_gpt2 (vocabulary_size =64002)\n",
    "model_gpt2.save_pretrained('/content/drive/MyDrive/BERT/save_modelGPT2/')\n",
    "task = {\"text-generation\": {\"do_sample\": True, \"max_length\": 56}} #edit output size\n",
    "configuration = GPT2Config(vocab_size=64002, n_positions=58, n_ctx=58,\n",
    "                           task_specific_params=task,\n",
    "                           eos_token_id = 2,\n",
    "                           bos_token_id = 0,\n",
    "                           pad_token_id = 1,\n",
    "                           sep_token_id = 2,\n",
    "                          #  eos_token_id=custokenizer.eos_token_id,\n",
    "                          #  bos_token_id=custokenizer.bos_token_id, \n",
    "                          #  pad_token_id=custokenizer.pad_token_id,\n",
    "                          #  sep_token_id=custokenizer.sep_token_id\n",
    "                           )\n",
    "poem = GPT2LMHeadModel(configuration)\n",
    "\n",
    "# Load weights of model_gpt2 ( random weights)\n",
    "load_model_gpt2 = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/BERT/save_modelGPT2/')\n",
    "poem.load_state_dict(load_model_gpt2.state_dict())\n",
    "#-----------Print process training ------------#\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from transformers import pipeline\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        if int(state.epoch)%10==0:\n",
    "            pipe = pipeline('text-generation', model=model, tokenizer=custokenizer, device=0)\n",
    "            with open(\"/content/drive/MyDrive/BERT/sample.txt\", \"a\") as f:\n",
    "                f.write(pipe('<s> tìm về một thuở hạ xưa')[0]['generated_text'])\n",
    "                f.write(\"\\n===========================================\\n\")\n",
    "                f.close()\n",
    " training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/BERT/gpt2-poem\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=100, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training  \n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    save_steps=5000, # after # steps model is saved \n",
    "    save_total_limit = 2, # delete other checkpoints\n",
    "    warmup_steps=5000,    # number of warmup steps for learning rate scheduler\n",
    "    # logging_dir='/content/drive/MyDrive/BERT/gpt2-poem/logs', # directory for storing logs\n",
    "    logging_steps=5000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "trainer = Trainer(\n",
    "    model=poem, # GPT2\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks = [PrinterCallback],\n",
    ")\n",
    "# -------Train and save model-----------#\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Load model saved-----------------#\n",
    "from transformers import pipeline\n",
    "poem = pipeline('text-generation', model=\"/content/drive/MyDrive/BERT/gpt2-poem\", tokenizer=custokenizer, config={'max_length':56})\n",
    "#Test\n",
    "a = poem('<s>cuộc sống')\n",
    "print(a[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
